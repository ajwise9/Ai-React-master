{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajwise9/Ai-React-master/blob/main/Copy_of_PriceGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PriceGPT\n",
        "\n",
        "Using an autoregressive transformer to predict changes in stock prices instead of  words."
      ],
      "metadata": {
        "id": "zXbmhXYEVcte"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0Eqh-yh0ZYSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhEEwM9BtfZl",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Settings\n",
        "\n",
        "#@markdown Device to run training and inference on:\n",
        "device = 'cuda' #@param ['cuda', 'cpu']\n",
        "#@markdown Stock ticker to use:\n",
        "stock_ticker = 'SPY' #@param {\"type\": \"string\"}\n",
        "#@markdown How much data to download:\n",
        "download_bars = 2000 #@param {\"type\": \"integer\"}\n",
        "#@markdown The amount of training examples\n",
        "training_bars = 1000 #@param {\"type\": \"integer\"}\n",
        "#@markdown How often to retrain:\n",
        "testing_bars = 100 #@param {\"type\": \"integer\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2P1QqiRuHBW"
      },
      "outputs": [],
      "source": [
        "#@title Download And Process Data\n",
        "\n",
        "import yfinance as yf\n",
        "import numpy as np\n",
        "\n",
        "yf_df = yf.download(stock_ticker, group_by='ticker')\n",
        "\n",
        "price = yf_df['Adj Close'].to_numpy()\n",
        "log_returns = np.diff(np.log(price))\n",
        "\n",
        "log_returns = log_returns[-download_bars:]\n",
        "\n",
        "print('downloaded %d bars'%len(log_returns))\n",
        "\n",
        "\n",
        "def generate_features_and_labels(log_returns, n_tokens):\n",
        "  features = []\n",
        "  labels = []\n",
        "\n",
        "  for feature_start in range(len(log_returns) - n_tokens):\n",
        "    feature_end = feature_start + n_tokens\n",
        "    features.append(log_returns[feature_start:feature_end].reshape((-1, 1)).copy())\n",
        "    labels.append(log_returns[feature_end])\n",
        "\n",
        "  features = np.array(features)\n",
        "  labels = np.array(labels)\n",
        "\n",
        "  return features, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zJ2RB_Zvj4H",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Define Model\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from positional_encodings.torch_encodings import PositionalEncoding1D\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "n_features = 1\n",
        "#@markdown Batch size for training and inference:\n",
        "batch_size = 1024 #@param {\"type\": \"integer\"}\n",
        "#@markdown How many epochs to train for:\n",
        "epochs = 500 #@param {\"type\": \"integer\"}\n",
        "#@markdown How many price bars will be visible to the model at a time:\n",
        "n_tokens = 100 #@param {\"type\": \"integer\"}\n",
        "#@markdown How many dimensions each token vector will have:\n",
        "d_model = 16 #@param {\"type\": \"integer\"}\n",
        "#@markdown How many heads to split attention into:\n",
        "n_heads = 1 #@param {\"type\": \"integer\"}\n",
        "#@markdown How many transformer layers to use:\n",
        "n_layers = 1 #@param {\"type\": \"integer\"}\n",
        "precision = torch.float32\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, n_tokens, d_model, n_heads):\n",
        "    super().__init__()\n",
        "\n",
        "    self.attention_norm = nn.LayerNorm((n_tokens, d_model))\n",
        "    self.attention = nn.MultiheadAttention(d_model, n_heads, batch_first=True)\n",
        "\n",
        "    self.mlp_norm = nn.LayerNorm((n_tokens, d_model))\n",
        "    self.mlp_linear1 = nn.Linear(d_model, d_model * 4)\n",
        "    self.mlp_activation = nn.GELU()\n",
        "    self.mlp_linear2 = nn.Linear(d_model * 4, d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x_attention_norm = self.attention_norm(x)\n",
        "    mask = nn.Transformer.generate_square_subsequent_mask(n_tokens, device=x.device)\n",
        "    x = x + self.attention(x_attention_norm, x_attention_norm, x_attention_norm, attn_mask=mask, is_causal=True)[0]\n",
        "\n",
        "    x_mlp_norm = self.mlp_norm(x)\n",
        "    y = self.mlp_linear1(x_mlp_norm)\n",
        "    y = self.mlp_activation(y)\n",
        "    y = self.mlp_linear2(y)\n",
        "    x = x + y\n",
        "\n",
        "    return x\n",
        "\n",
        "class AutoregressiveTransformer(nn.Module):\n",
        "  def __init__(self, n_tokens=n_tokens, d_model=d_model, n_heads=n_heads, n_layers=n_layers):\n",
        "    super().__init__()\n",
        "\n",
        "    self.projection = nn.Linear(n_features, d_model)\n",
        "    self.projection_activation = nn.GELU()\n",
        "    self.embedding = PositionalEncoding1D(d_model)\n",
        "    self.norm = nn.LayerNorm(normalized_shape=(n_tokens, d_model))\n",
        "\n",
        "    decoder_layers = []\n",
        "    for i in range(n_layers):\n",
        "      decoder_layers.append(DecoderLayer(n_tokens, d_model, n_heads))\n",
        "    self.decoder_layers = nn.ModuleList(decoder_layers)\n",
        "\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.readout = nn.Linear(n_tokens * d_model, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.projection(x)\n",
        "    x = self.projection_activation(x)\n",
        "    x = x + self.embedding(x)\n",
        "    x = self.norm(x)\n",
        "\n",
        "    for layer in self.decoder_layers:\n",
        "      x = layer(x)\n",
        "\n",
        "    x = self.flatten(x)\n",
        "    x = self.readout(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def make_and_train_model(features, labels, log_losses=False):\n",
        "  model = AutoregressiveTransformer().to(precision).to(device)\n",
        "  optim = AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "  features_tensor = torch.tensor(features).to(precision)\n",
        "  labels_tensor = torch.tensor(labels).to(precision)\n",
        "\n",
        "  dataset = TensorDataset(features_tensor, labels_tensor)\n",
        "  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True, pin_memory_device=device)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    losses = []\n",
        "\n",
        "    for batch_idx, (feature, label) in enumerate(dataloader):\n",
        "      optim.zero_grad()\n",
        "\n",
        "      model_input = feature.to(device)\n",
        "\n",
        "      out = model(model_input)\n",
        "\n",
        "      loss = torch.mean((out[:, 0] - label.to(device)) ** 2)\n",
        "\n",
        "      loss.backward()\n",
        "      optim.step()\n",
        "\n",
        "      losses.append(loss.cpu().detach().numpy())\n",
        "\n",
        "    if log_losses:\n",
        "      print('epoch %d, loss: %.3f' % (epoch, np.mean(losses)))\n",
        "\n",
        "  return model\n",
        "\n",
        "def batch_predict(model, features):\n",
        "  features_tensor = torch.tensor(features)\n",
        "  predictions = np.array([])\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch_start in range(0, features_tensor.shape[0], batch_size):\n",
        "      model_input = features_tensor[batch_start:batch_start + batch_size].to(precision).to(device)\n",
        "\n",
        "      prediction = model(model_input)\n",
        "      prediction = prediction[:, 0].cpu().numpy()\n",
        "      predictions = np.concatenate((predictions, prediction))\n",
        "\n",
        "  return predictions\n",
        "\n",
        "def make_and_train_ensemble(n_models, features, labels, log_losses=False):\n",
        "  models = []\n",
        "  for i in range(n_models):\n",
        "    models.append(make_and_train_model(features, labels, log_losses=log_losses))\n",
        "  return models\n",
        "\n",
        "def batch_predict_ensemble(models, features):\n",
        "  result = np.zeros(len(features))\n",
        "  for model in models:\n",
        "    result += batch_predict(model, features)\n",
        "  result /= len(models)\n",
        "  return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7ixac0cxowr"
      },
      "outputs": [],
      "source": [
        "#@title Sliding Window Backtest\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "#@markdown Log the train loss of each epoch:\n",
        "log_losses = False #@param {\"type\": \"boolean\"}\n",
        "#@markdown Number of models for ensembling:\n",
        "n_models = 3 #@param {\"type\": \"integer\"}\n",
        "\n",
        "\n",
        "\n",
        "%pylab inline\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "all_returns = np.array([])\n",
        "all_strategy_returns = np.array([])\n",
        "all_allocations = np.array([])\n",
        "\n",
        "for train_start in tqdm(range(0, len(log_returns) - training_bars, testing_bars)):\n",
        "  train_end = train_start + training_bars\n",
        "  test_start = train_end - n_tokens\n",
        "  test_end = train_end + testing_bars\n",
        "\n",
        "  train_features, train_labels = generate_features_and_labels(log_returns[train_start:train_end], n_tokens)\n",
        "\n",
        "  mean_feature = np.mean(train_features)\n",
        "  std_feature = np.std(train_features)\n",
        "\n",
        "  train_features -= mean_feature\n",
        "  train_features /= std_feature\n",
        "  train_labels -= mean_feature\n",
        "  train_labels /= std_feature\n",
        "\n",
        "  models = make_and_train_ensemble(n_models, train_features, train_labels, log_losses=log_losses)\n",
        "\n",
        "  train_predictions = batch_predict_ensemble(models, train_features)\n",
        "  mean_prediction = np.mean(train_predictions)\n",
        "  std_prediction = np.std(train_predictions)\n",
        "\n",
        "  test_features, test_labels = generate_features_and_labels(log_returns[test_start:test_end], n_tokens)\n",
        "\n",
        "  test_returns = np.exp(test_labels) - 1\n",
        "\n",
        "  test_features -= mean_feature\n",
        "  test_features /= std_feature\n",
        "  test_labels -= mean_feature\n",
        "  test_labels /= std_feature\n",
        "\n",
        "  # make sure no data is leaking\n",
        "  # the last label should become a feature when moving on to testing\n",
        "  assert test_features[0, -1, 0] == train_labels[-1]\n",
        "\n",
        "  test_predictions = batch_predict_ensemble(models, test_features)\n",
        "\n",
        "  test_predictions -= mean_prediction\n",
        "  test_predictions /= std_prediction\n",
        "\n",
        "  # buy if better than average\n",
        "  allocation = 1 * (test_predictions > 0)\n",
        "\n",
        "  # buy if better than anverage, short if worse than average\n",
        "  # allocation = np.sign(test_predictions)\n",
        "\n",
        "  # proportional\n",
        "  # allocation = 0.5 * test_predictions\n",
        "\n",
        "  strategy_returns = allocation * test_returns\n",
        "\n",
        "  all_returns = np.concatenate((all_returns, test_returns))\n",
        "  all_strategy_returns = np.concatenate((all_strategy_returns, strategy_returns))\n",
        "  all_allocations = np.concatenate((all_allocations, allocation))\n",
        "\n",
        "\n",
        "plt.plot(np.cumprod(1 + all_returns), label='baseline equity')\n",
        "plt.plot(np.cumprod(1 + all_strategy_returns), label='strategy equity')\n",
        "# plt.yscale('log')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# plt.plot(np.cumprod(1 + all_returns), label='baseline equity')\n",
        "# plt.plot(np.cumprod(1 + all_strategy_returns / np.std(all_strategy_returns) * np.std(all_returns)), label='strategy equity (matched risk)')\n",
        "# plt.legend()\n",
        "# plt.show()\n",
        "\n",
        "sharpe = np.sqrt(252) * np.mean(all_strategy_returns) / np.std(all_strategy_returns)\n",
        "\n",
        "print('baseline sharpe ratio:', np.sqrt(252) * np.mean(all_returns) / np.std(all_returns))\n",
        "print('strategy sharpe ratio:', sharpe)\n",
        "\n",
        "# p-value testing\n",
        "\n",
        "beat_by_random = []\n",
        "\n",
        "for i in range(10000):\n",
        "  random_order = np.random.permutation(len(all_allocations))\n",
        "  random_returns = all_returns * all_allocations[random_order]\n",
        "  random_sharpe = np.sqrt(252) * np.mean(random_returns) / np.std(random_returns)\n",
        "  beat_by_random.append(random_sharpe > sharpe)\n",
        "\n",
        "p_value = np.mean(beat_by_random)\n",
        "print('p-value:', p_value)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Make Live Prediction\n",
        "#@markdown The output is normalized to the distribution of the model's past predictions.\n",
        "#@markdown\n",
        "#@markdown Number of models for ensembling:\n",
        "n_models = 3 #@param {\"type\": \"integer\"}\n",
        "\n",
        "train_features, train_labels = generate_features_and_labels(log_returns[-training_bars:], n_tokens)\n",
        "\n",
        "mean_feature = np.mean(train_features)\n",
        "std_feature = np.std(train_features)\n",
        "\n",
        "train_features -= mean_feature\n",
        "train_features /= std_feature\n",
        "train_labels -= mean_feature\n",
        "train_labels /= std_feature\n",
        "\n",
        "models = make_and_train_ensemble(n_models, train_features, train_labels)\n",
        "\n",
        "train_predictions = batch_predict_ensemble(models, train_features)\n",
        "mean_prediction = np.mean(train_predictions)\n",
        "std_prediction = np.std(train_predictions)\n",
        "\n",
        "live_features = log_returns[-n_tokens:].copy().reshape((1, -1, 1))\n",
        "\n",
        "live_features -= mean_feature\n",
        "live_features /= std_feature\n",
        "\n",
        "live_prediction = batch_predict_ensemble(models, live_features)[0]\n",
        "live_prediction -= mean_prediction\n",
        "live_prediction /= std_prediction\n",
        "\n",
        "print('live prediction (z-score):', live_prediction)"
      ],
      "metadata": {
        "id": "Opj5_oD3_mXR",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}